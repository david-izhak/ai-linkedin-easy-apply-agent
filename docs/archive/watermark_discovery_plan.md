# Поэтапный план внедрения “watermark”-сборщика с лимитом без потерь

## [deprecated] Этап 1. Расширение схемы БД: discovery_state
- **Описание**: Добавить таблицу состояния дискавери для хранения маркеров прогресса по каждому уникальному запросу.
- **Задачи**:
  - [deprecated] Создать таблицу `discovery_state(search_key UNIQUE, last_seen_max_job_id, last_complete_sweep_before_id, updated_at)`.
  - Реализовать CRUD: чтение по `search_key`, UPSERT с монотонным `last_seen_max_job_id`.
- **Ресурсы и инструменты**:
  - Файл: `core/database.py`, SQLite (`sqlite3`), функция `setup_database()`.
- **Результаты**:
  - Устойчивая таблица состояний; можно безопасно читать/обновлять маркеры прогресса.

## Этап 2. Генерация стабильного search_key
- **Описание**: Определить детерминированный ключ состояния из параметров поиска.
- **Задачи**:
  - Собрать параметры (`keywords`, `geoId`, `distance`, `f_WT`, `f_TPR`, `sortBy`, ...).
  - Отсортированно сериализовать (JSON/строка) и получить SHA256 → `search_key`.
  - Вынести helper `make_search_key(params)` (например, в `core/utils.py`).
- **Ресурсы и инструменты**:
  - Файлы: `actions/fetch_jobs.py`, `core/utils.py`; модули `json`, `hashlib`.
- **Результаты**:
  - Для одинаковых фильтров формируется один и тот же ключ, состояния изолированы.

## Этап 3. Цикл дискавери: набирать именно L unseen
- **Описание**: Продолжать пагинацию, пока не наберём лимит новых для БД вакансий.
- **Задачи**:
  - [deprecated] В `fetch_job_links_user`: загрузить `discovery_state` по `search_key`.
  - На каждой странице собирать `(job_id, link, title, company)`.
  - Исключать уже существующие id (предварительно или полагаясь на `INSERT OR IGNORE`).
  - Накопить `unseen` до L или до исчерпания выдачи.
- **Ресурсы и инструменты**:
  - Файл: `actions/fetch_jobs.py`; опционально batch lookup в `core/database.py`.
- **Результаты**:
  - За запуск собирается до L действительно новых вакансий, даже при большом числе дублей.

## Этап 4. Обновление маркеров прогресса
- **Описание**: Корректно двигать верхнюю и нижнюю границы просмотра.
- **Задачи**:
  - После вставки:
    - `last_seen_max_job_id = max(prev, max(inserted_ids))` (если были вставки).
    - `last_complete_sweep_before_id = min(all_observed_ids_this_run)` — минимальный id, встреченный в самом глубоком пролистывании (включая дубликаты).
  - Если выдача исчерпана — считать проход завершённым до этой границы.
- **Ресурсы и инструменты**:
  - Файлы: `actions/fetch_jobs.py` (сбор наблюдаемых id), `core/database.py` (UPSERT state).
- **Результаты**:
  - Состояние отражает прогресс как по «новым сверху», так и по «добору хвоста».

## Этап 5. Тестирование (unit + integration)
- **Описание**: Проверить лимиты, дубляжи и добор хвоста.
- **Задачи**:
  - Unit: сценарий с 90% дублей на первых страницах, лимит L=10 → добираем ровно 10 unseen.
  - Unit: проверка монотонности `last_seen_max_job_id` и корректности `last_complete_sweep_before_id`.
  - [deprecated] Integration: CRUD `discovery_state`, идемпотентные вставки в `vacancies`, продвижение маркеров.
- **Ресурсы и инструменты**:
  - Файлы: `tests/unit/actions/test_discovery_watermark.py`, `tests/integration/test_discovery_state_flow.py`.
  - Pytest, моки Playwright (для unit).
- **Результаты**:
  - Автоматические проверки подтверждают отсутствие потерь при лимите и корректные маркеры.

## Этап 6. Миграция/инициализация
- **Описание**: Идёмпотентное создание таблицы на существующих развёртываниях.
- **Задачи**:
  - [deprecated] Добавить DDL `discovery_state` в `setup_database()`.
  - Проверить первый запуск на уже существующей БД.
- **Ресурсы и инструменты**:
  - Файл: `core/database.py`.
- **Результаты**:
  - Приложение само создаёт таблицу при первом старте без ручных миграций.

## Этап 7. Документация и конфиг
- **Описание**: Объяснить схему и поведение лимита.
- **Задачи**:
  - Обновить гайды (`QUICK_START_GUIDE.md`, `docs/6_configuration_and_run.md`) — поведение лимита и watermark.
  - [deprecated] Добавить/уточнить `docs/discovery_state.md` (схема, инварианты, UPSERT, логирование).
- **Ресурсы и инструменты**:
  - Файлы: `docs/discovery_state.md`, `QUICK_START_GUIDE.md`.
- **Результаты**:
  - Команда понимает, как работает сборщик и как настраивать `MAX_JOBS_TO_DISCOVER`.

## Этап 8. Логирование и наблюдаемость
- **Описание**: Диагностика поведения в проде.
- **Задачи**:
  - Логировать: `search_key`, число страниц, объём `unseen`, долю дублей, обновлённые маркеры.
  - Уровни: INFO — сводка; DEBUG — подробности.
- **Ресурсы и инструменты**:
  - Файлы: `actions/fetch_jobs.py`, `core/database.py`.
- **Результаты**:
  - Прозрачная трассировка, упрощённый разбор инцидентов.

## Этап 9. Постепенный rollout
- **Описание**: Безопасный ввод изменений.
- **Задачи**:
  - Запустить с малым лимитом (например, 5), проверить логи/БД.
  - Убедиться в отсутствии дублей и продвижении маркеров.
  - Увеличить лимит до рабочего значения.
- **Ресурсы и инструменты**:
  - Логи, `jobs.db`.
- **Результаты**:
  - Подтверждение, что вакансии не теряются; хвост добирается.

## Этап 10. Рефакторинг и оптимизации
- **Описание**: Улучшения производительности.
- **Задачи**:
  - Batch-проверка существующих id: `SELECT id FROM vacancies WHERE id IN (...)` пачками.
  - Деление лимита: часть на «новые сверху», часть на «добор хвоста» при высоком потоке.
- **Ресурсы и инструменты**:
  - Файлы: `actions/fetch_jobs.py`, `core/database.py`.
- **Результаты**:
  - Стабильная производительность и предсказуемая «доборность» хвоста.
